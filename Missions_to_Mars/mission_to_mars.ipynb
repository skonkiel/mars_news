{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "import requests\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize splinter\n",
    "executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_data = {} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Scrape the NASA Mars News Site and collect the latest News Title and Paragraph Text. \n",
    "Assign the text to variables that you can reference later.'''\n",
    "\n",
    "url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "browser.visit(url)\n",
    "\n",
    "# Retrieve page & parse with BeautifulSoup\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)\n",
    "\n",
    "news_items = soup.find('ul', class_='item_list')\n",
    "mars_data['news_title'] = news_items.find('div', class_='content_title').text\n",
    "mars_data['news_para'] = news_items.find('div', class_='article_teaser_body').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(url)\n",
    "\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Use splinter to navigate the site and find the image url for the current Featured Mars Image and \n",
    "# assign the url string to a variable called featured_image_url.\n",
    "s = soup.find('article', class_='carousel_item')['style']\n",
    "url = s.split(\"'\")[1]\n",
    "mars_data['featured_image_url'] = 'https://www.jpl.nasa.gov' + url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Weather (Twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Visit the Mars Weather twitter account and scrape the latest Mars weather tweet from the page. \n",
    "Save the tweet text for the weather report as a variable called mars_weather.\n",
    "'''\n",
    "\n",
    "url = 'https://twitter.com/marswxreport?lang=en'\n",
    "\n",
    "html_text = requests.get(url).text\n",
    "soup = BeautifulSoup(html_text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_weather = soup.find_all('p', class_='tweet-text')[0].text\n",
    "mars_weather = mars_weather.split('pic.twitter.com')\n",
    "mars_data['mars_weather']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the Mars Facts webpage here and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "r = requests.get(\"https://space-facts.com/mars/\")\n",
    "soup = BeautifulSoup(r.content,'lxml')\n",
    "table = soup.find('table', id='tablepress-p-mars-no-2')\n",
    "df = pd.read_html(str(table))\n",
    "df = df[0]\n",
    "\n",
    "df.columns = ['description', 'value']\n",
    "df.set_index(keys=['description'], inplace=True)\n",
    "\n",
    "# Use Pandas to convert the data to a HTML table string.\n",
    "html = df.to_html()\n",
    "mars_data['html_table'] = html.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\")\n",
    "soup = BeautifulSoup(r.content,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all('a', class_='itemLink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere_image_urls = []\n",
    "\n",
    "for link in links:\n",
    "    hemisphere = {}\n",
    "    r = requests.get(\"https://astrogeology.usgs.gov/\" + link['href'])\n",
    "    soup = BeautifulSoup(r.content,'lxml')\n",
    "    l = soup.find('li')\n",
    "    link = l.contents[0]\n",
    "    img_url = link['href']\n",
    "    title = soup.title.text\n",
    "    title = title.split(\"|\")[0]\n",
    "    hemisphere[\"title\"] = title\n",
    "    hemisphere[\"img_url\"] = img_url\n",
    "    hemisphere_image_urls.append(hemisphere)\n",
    "    time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
